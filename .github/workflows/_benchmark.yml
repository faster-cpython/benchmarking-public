# Generated file: !!! DO NOT EDIT !!!
---
name: _benchmark

on:
  workflow_call:
    inputs:
      fork:
        description: Fork of cpython to benchmark
        type: string
      ref:
        description: Branch, tag or (full) SHA commit to benchmark
        type: string
      machine:
        description: Machine to run on
        type: string
      benchmarks:
        description: Benchmarks to run (comma-separated; empty runs all 
          benchmarks)
        type: string
      pgo:
        description: Build with PGO
        type: boolean
      force:
        description: Rerun and replace results if commit already exists
        type: boolean
      perf:
        description: Collect Linux perf profiling data (Linux only)
        type: boolean

      tier2:
        description: tier 2 interpreter
        type: boolean
        default: false
      jit:
        description: JIT
        type: boolean
        default: false
      nogil:
        description: free threading
        type: boolean
        default: false
      tailcall:
        description: tail-calling interpreter
        type: boolean
        default: false
  workflow_dispatch:
    inputs:
      fork:
        description: Fork of cpython to benchmark
        type: string
        default: python
      ref:
        description: Branch, tag or (full) SHA commit to benchmark
        type: string
        default: main
      machine:
        description: Machine to run on
        default: linux-amd64
        type: choice
        options:
        - linux-x86_64-linux
        - linux-x86_64-linux_clang
        - linux-x86_64-pythonperf2
        - linux-x86_64-pythonperf2_clang
        - linux-aarch64-arminc
        - linux-aarch64-arminc_clang
        - darwin-arm64-darwin
        - darwin-arm64-darwin_clang
        - windows-x86_64-pythonperf1
        - windows-x86_64-pythonperf1_clang
        - windows-x86-pythonperf1_win32
        - windows-x86_64-pythonperf1_win32_clang
        - all
      benchmarks:
        description: Benchmarks to run (comma-separated; empty runs all 
          benchmarks)
        type: string
      force:
        description: Rerun and replace results if commit already exists
        type: boolean
      perf:
        description: Collect Linux perf profiling data (Linux only)
        type: boolean

      tier2:
        description: tier 2 interpreter
        type: boolean
        default: false
      jit:
        description: JIT
        type: boolean
        default: false
      nogil:
        description: free threading
        type: boolean
        default: false
      tailcall:
        description: tail-calling interpreter
        type: boolean
        default: false
jobs:
  benchmark-linux-x86_64-linux:
    runs-on: [self-hosted, linux, bare-metal, linux-x86_64-linux]
    timeout-minutes: 1440

    steps:
    - name: Setup environment
      run: |-
        echo "BENCHMARK_MACHINE_NICKNAME=linux" >> $GITHUB_ENV
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - uses: fregante/setup-git-user@v2
    - name: Setup system Python
      if: ${{ runner.arch == 'X64' }}
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    - name: Building Python and running pyperformance
      run: |
        python workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }} ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} ${{ inputs.perf && '--perf' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      if: ${{ !inputs.perf }}
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Adding data to repo
      if: ${{ !inputs.perf }}
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload benchmark artifacts
      if: ${{ !inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true
    - name: Upload perf artifacts
      if: ${{ inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: perf
        path: |
          profiling/results

    if: ${{ (inputs.machine == 'linux-x86_64-linux' || inputs.machine == 
      '__really_all' || inputs.machine == 'all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-linux-x86_64-linux_clang:
    runs-on: [self-hosted, linux, bare-metal, linux-x86_64-linux]
    timeout-minutes: 1440

    steps:
    - name: Setup environment
      run: |-
        echo "CC=$(which clang-19)" >> $GITHUB_ENV
        echo "LLVM_AR=$(which llvm-ar-19)" >> $GITHUB_ENV
        echo "LLVM_PROFDATA=$(which llvm-profdata-19)" >> $GITHUB_ENV
        echo "BENCHMARK_MACHINE_NICKNAME=linux_clang" >> $GITHUB_ENV
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - uses: fregante/setup-git-user@v2
    - name: Setup system Python
      if: ${{ runner.arch == 'X64' }}
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    - name: Building Python and running pyperformance
      run: |
        python workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }} ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} ${{ inputs.perf && '--perf' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      if: ${{ !inputs.perf }}
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Adding data to repo
      if: ${{ !inputs.perf }}
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload benchmark artifacts
      if: ${{ !inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true
    - name: Upload perf artifacts
      if: ${{ inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: perf
        path: |
          profiling/results

    if: ${{ (inputs.machine == 'linux-x86_64-linux_clang' || inputs.machine == 
      '__really_all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-linux-x86_64-pythonperf2:
    runs-on: [self-hosted, linux, bare-metal, linux-x86_64-pythonperf2]
    timeout-minutes: 1440

    steps:
    - name: Setup environment
      run: |-
        echo "BENCHMARK_MACHINE_NICKNAME=pythonperf2" >> $GITHUB_ENV
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - uses: fregante/setup-git-user@v2
    - name: Setup system Python
      if: ${{ runner.arch == 'X64' }}
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    - name: Building Python and running pyperformance
      run: |
        python workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }} ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} ${{ inputs.perf && '--perf' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      if: ${{ !inputs.perf }}
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Adding data to repo
      if: ${{ !inputs.perf }}
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload benchmark artifacts
      if: ${{ !inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true
    - name: Upload perf artifacts
      if: ${{ inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: perf
        path: |
          profiling/results

    if: ${{ (inputs.machine == 'linux-x86_64-pythonperf2' || inputs.machine == 
      '__really_all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-linux-x86_64-pythonperf2_clang:
    runs-on: [self-hosted, linux, bare-metal, linux-x86_64-pythonperf2]
    timeout-minutes: 1440

    steps:
    - name: Setup environment
      run: |-
        echo "cc=$(which clang-19)" >> $GITHUB_ENV
        echo "llvm_ar=$(which llvm-ar-19)" >> $GITHUB_ENV
        echo "llvm_profdata=$(which llvm-profdata-19)" >> $GITHUB_ENV
        echo "BENCHMARK_MACHINE_NICKNAME=pythonperf2_clang" >> $GITHUB_ENV
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - uses: fregante/setup-git-user@v2
    - name: Setup system Python
      if: ${{ runner.arch == 'X64' }}
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    - name: Building Python and running pyperformance
      run: |
        python workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }} ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} ${{ inputs.perf && '--perf' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      if: ${{ !inputs.perf }}
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Adding data to repo
      if: ${{ !inputs.perf }}
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload benchmark artifacts
      if: ${{ !inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true
    - name: Upload perf artifacts
      if: ${{ inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: perf
        path: |
          profiling/results

    if: ${{ (inputs.machine == 'linux-x86_64-pythonperf2_clang' || 
      inputs.machine == '__really_all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-linux-aarch64-arminc:
    runs-on: [self-hosted, linux, bare-metal, linux-aarch64-arminc]
    timeout-minutes: 1440

    steps:
    - name: Setup environment
      run: |-
        echo "BENCHMARK_MACHINE_NICKNAME=arminc" >> $GITHUB_ENV
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - uses: fregante/setup-git-user@v2
    - name: Setup system Python
      if: ${{ runner.arch == 'X64' }}
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    - name: Building Python and running pyperformance
      run: |
        python workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }} ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} ${{ inputs.perf && '--perf' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      if: ${{ !inputs.perf }}
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Adding data to repo
      if: ${{ !inputs.perf }}
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload benchmark artifacts
      if: ${{ !inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true
    - name: Upload perf artifacts
      if: ${{ inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: perf
        path: |
          profiling/results

    if: ${{ (inputs.machine == 'linux-aarch64-arminc' || inputs.machine == 
      '__really_all' || inputs.machine == 'all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-linux-aarch64-arminc_clang:
    runs-on: [self-hosted, linux, bare-metal, linux-x86_64-arminc]
    timeout-minutes: 1440

    steps:
    - name: Setup environment
      run: |-
        echo "cc=$(which clang-19)" >> $GITHUB_ENV
        echo "llvm_ar=$(which llvm-ar-19)" >> $GITHUB_ENV
        echo "llvm_profdata=$(which llvm-profdata-19)" >> $GITHUB_ENV
        echo "BENCHMARK_MACHINE_NICKNAME=arminc_clang" >> $GITHUB_ENV
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - uses: fregante/setup-git-user@v2
    - name: Setup system Python
      if: ${{ runner.arch == 'X64' }}
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    - name: Building Python and running pyperformance
      run: |
        python workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }} ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} ${{ inputs.perf && '--perf' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      if: ${{ !inputs.perf }}
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Adding data to repo
      if: ${{ !inputs.perf }}
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload benchmark artifacts
      if: ${{ !inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true
    - name: Upload perf artifacts
      if: ${{ inputs.perf }}
      uses: actions/upload-artifact@v4
      with:
        name: perf
        path: |
          profiling/results

    if: ${{ (inputs.machine == 'linux-aarch64-arminc_clang' || inputs.machine ==
      '__really_all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-darwin-arm64-darwin:
    runs-on: [self-hosted, macos, bare-metal, darwin-arm64-darwin]

    steps:
    - name: Setup environment
      run: |-
        echo "BENCHMARK_MACHINE_NICKNAME=darwin" >> $GITHUB_ENV
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - name: Building Python and running pyperformance
      run: |
        python3 workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }} ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Add data to repo
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true
    if: ${{ (inputs.machine == 'darwin-arm64-darwin' || inputs.machine == 
      '__really_all' || inputs.machine == 'all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-darwin-arm64-darwin_clang:
    runs-on: [self-hosted, macos, bare-metal, darwin-arm64-darwin]

    steps:
    - name: Setup environment
      run: |-
        echo "PATH=$(brew --prefix llvm)/bin:$PATH" >> $GITHUB_ENV
        echo "CC=$(brew --prefix llvm)/bin/clang" >> $GITHUB_ENV
        echo "LDFLAGS=-L$(brew --prefix llvm)/lib" >> $GITHUB_ENV
        echo "CFLAGS=-L$(brew --prefix llvm)/include" >> $GITHUB_ENV
        echo "BENCHMARK_MACHINE_NICKNAME=darwin_clang" >> $GITHUB_ENV
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - name: Building Python and running pyperformance
      run: |
        python3 workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }} ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Add data to repo
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true
    if: ${{ (inputs.machine == 'darwin-arm64-darwin_clang' || inputs.machine == 
      '__really_all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-windows-x86_64-pythonperf1:
    runs-on: [self-hosted, windows, bare-metal, windows-x86_64-pythonperf1]

    steps:
      # Tell git to checkout repos with symlinks (required by pyston
      # benchmarks).
      # Requires "Developer Mode" switched on in Windows 10/11
    - name: Setup environment
      run: |-
        echo "BUILD_DEST=PCBuild/amd64" >> $env:GITHUB_ENV
        echo "BENCHMARK_MACHINE_NICKNAME=pythonperf1" >> $env:GITHUB_ENV
    - name: Enable symlinks for git
      run: |
        git config --global core.symlinks true
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - name: Building Python and running pyperformance
      run: |
        py workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} "${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }}" ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Add data to repo
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true

    if: ${{ (inputs.machine == 'windows-x86_64-pythonperf1' || inputs.machine ==
      '__really_all' || inputs.machine == 'all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-windows-x86_64-pythonperf1_clang:
    runs-on: [self-hosted, windows, bare-metal, windows-x86_64-pythonperf1]

    steps:
      # Tell git to checkout repos with symlinks (required by pyston
      # benchmarks).
      # Requires "Developer Mode" switched on in Windows 10/11
    - name: Setup environment
      run: |-
        echo "BUILD_DEST=PCBuild/amd64" >> $env:GITHUB_ENV
        echo "PYTHON_CONFIGURE_FLAGS=`"/p:PlatformToolset`=clangcl`" `"/p:LLVMInstallDir`=C:\Program Files\LLVM`" `"/p:LLVMToolsVersion`=19.1.6`"" >> $env:GITHUB_ENV
        echo "BENCHMARK_MACHINE_NICKNAME=pythonperf1_clang" >> $env:GITHUB_ENV
    - name: Enable symlinks for git
      run: |
        git config --global core.symlinks true
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - name: Building Python and running pyperformance
      run: |
        py workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} "${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }}" ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Add data to repo
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true

    if: ${{ (inputs.machine == 'windows-x86_64-pythonperf1_clang' || 
      inputs.machine == '__really_all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-windows-x86-pythonperf1_win32:
    runs-on: [self-hosted, windows, bare-metal, windows-x86_64-pythonperf1]

    steps:
      # Tell git to checkout repos with symlinks (required by pyston
      # benchmarks).
      # Requires "Developer Mode" switched on in Windows 10/11
    - name: Setup environment
      run: |-
        echo "BUILD_FLAGS=-p Win32" >> $env:GITHUB_ENV
        echo "BUILD_DEST=PCBuild/win32" >> $env:GITHUB_ENV
        echo "BENCHMARK_MACHINE_NICKNAME=pythonperf1_win32" >> $env:GITHUB_ENV
    - name: Enable symlinks for git
      run: |
        git config --global core.symlinks true
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - name: Building Python and running pyperformance
      run: |
        py workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} "${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }}" ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Add data to repo
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true

    if: ${{ (inputs.machine == 'windows-x86-pythonperf1_win32' || inputs.machine
      == '__really_all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
  benchmark-windows-x86_64-pythonperf1_win32_clang:
    runs-on: [self-hosted, windows, bare-metal, windows-x86_64-pythonperf1]

    steps:
      # Tell git to checkout repos with symlinks (required by pyston
      # benchmarks).
      # Requires "Developer Mode" switched on in Windows 10/11
    - name: Setup environment
      run: |-
        echo "BUILD_DEST=PCBuild/win32" >> $env:GITHUB_ENV
        echo "BUILD_FLAGS=-p Win32" >> $env:GITHUB_ENV
        echo "PYTHON_CONFIGURE_FLAGS=`"/p:PlatformToolset`=clangcl`" `"/p:LLVMInstallDir`=C:\Program Files\LLVM`" `"/p:LLVMToolsVersion`=19.1.6`"" >> $env:GITHUB_ENV
        echo "BENCHMARK_MACHINE_NICKNAME=pythonperf1_win32_clang" >> $env:GITHUB_ENV
    - name: Enable symlinks for git
      run: |
        git config --global core.symlinks true
    - name: Checkout benchmarking
      uses: actions/checkout@v4
    - name: git gc
      run: |
        git gc
    - name: Building Python and running pyperformance
      run: |
        py workflow_bootstrap.py ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} ${{ inputs.benchmarks || 'all' }} "${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ inputs.tailcall == true && 'tailcall' || '' }}" ${{ inputs.force && '--force' || '' }} ${{ inputs.pgo && '--pgo' || '' }} --run_id ${{ github.run_id }}
      # Pull again, since another job may have committed results in the meantime
    - name: Pull benchmarking
      run: |
        # Another benchmarking task may have created results for the same
        # commit while the above was running. This "magic" incantation means
        # that any local results for this commit will override anything we
        # just pulled in in that case.
        git pull -s recursive -X ours --autostash --rebase
    - name: Add data to repo
      uses: EndBug/add-and-commit@v9
      with:
        add: results
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      with:
        name: benchmark
        path: |
          benchmark.json
        overwrite: true

    if: ${{ (inputs.machine == 'windows-x86_64-pythonperf1_win32_clang' || 
      inputs.machine == '__really_all') }}
    env:
      flags: ${{ inputs.tier2 == true && 'tier2' || '' }},${{ inputs.jit == true
        && 'jit' || '' }},${{ inputs.nogil == true && 'nogil' || '' }},${{ 
        inputs.tailcall == true && 'tailcall' || '' }}
