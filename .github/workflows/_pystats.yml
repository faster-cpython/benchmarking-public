---
name: _pystats

env:
  PYPERFORMANCE_HASH: 2e053ce183aca6f5dce64b071c90dd59ebadb065
  PYSTON_BENCHMARKS_HASH: d4868ff7825f3996e0005197643ed56eba4fb567

"on":
  workflow_dispatch:
    inputs:
      fork:
        description: 'Fork of cpython to benchmark'
        type: string
        default: python
      ref:
        description: 'Branch, tag or (full) SHA commit to benchmark'
        type: string
        default: main
      benchmarks:
        description: 'Benchmarks to run (comma-separated; empty runs all benchmarks)'
        type: string
      dry_run:
        description: 'Dry run: Do not commit to the repo'
        type: boolean
        default: false
      force:
        description: 'Rerun and replace results if commit already exists'
        type: boolean

  workflow_call:
    inputs:
      fork:
        description: 'Fork of cpython to benchmark'
        type: string
      ref:
        description: 'Branch, tag or (full) SHA commit to benchmark'
        type: string
      benchmarks:
        description: 'Benchmarks to run (comma-separated; empty runs all benchmarks)'
        type: string
      dry_run:
        description: 'Dry run: Do not commit to the repo'
        type: boolean
      force:
        description: 'Rerun and replace results if commit already exists'
        type: boolean

jobs:
  collect-stats:
    runs-on: [self-hosted, linux, azure]
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: pip
      - uses: actions/checkout@v3
        with:
          repository: ${{ inputs.fork }}/cpython
          ref: ${{ inputs.ref }}
          path: cpython
      - name: Should we run?
        if: ${{ always() }}
        id: should_run
        run: |
          python scripts/should_run.py ${{ inputs.force }} ${{ inputs.fork }} ${{ inputs.ref }} all >> $GITHUB_OUTPUT
      - uses: actions/checkout@v3
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        with:
          repository: pyston/python-macrobenchmarks
          path: pyston-benchmarks
          ref: ${{ env.PYSTON_BENCHMARKS_HASH }}
      - uses: actions/checkout@v3
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        with:
          repository: python/pyperformance
          path: pyperformance
          ref: ${{ env.PYPERFORMANCE_HASH }}
      - name: Create pystats directory
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          # If we don't do this, stats are printed to the console
          rm -rf /tmp/py_stats
          mkdir /tmp/py_stats
      - name: Compile
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          cd cpython
          ./configure --enable-pystats --prefix=$PWD/install
          make -j4
          make install
      - name: Install pyperformance into the system python
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          python -m pip install ./pyperformance
      - name: Running pyperformance
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          python scripts/run_benchmarks.py pystats cpython/python ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.benchmarks || 'all' }} false
      - name: pull
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          # Another benchmarking task may have created results for the same
          # commit while the above was running. This "magic" incantation means
          # that any local results for this commit will override anything we
          # just pulled in in that case.
          git pull -s recursive -X ours --autostash --rebase
      - name: Add to repo
        if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run }}
        uses: EndBug/add-and-commit@v9
        with:
          add: results
